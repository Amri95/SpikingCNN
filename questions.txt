
 Is it possible, known all the parameters given to a tf.nn.conv2d(..) operation, to go backward and compute the indexes that correspond to all the squares in the input tensor that has been considered for a the convolution of a single square in the output tensor ?

 Is it correct, theoretically to use this proportions for the learning training and testing dataset ? In particular, the training and testing sets are almost the same size, could it be better to have a training set bigger than the testing set ?

 We will proceed to process the dataset to give to the classifier with a normalization.


 Which kind of classifier could be more efficient in this kind of classification problem ? Binary classification, 20 labels in input, supervised.

 Which metrics should we extract from the classifier to deduce the overall result of the approach ? loss ?


 There is an intresting warning using the tf.estimator.BaselineClassifier that says:
 Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to "careful interpolation" instead


